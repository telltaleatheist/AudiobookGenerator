Ultimate RVC CLI Commands Reference

Main Commands
urvc train - Train voice models using RVC
  populate-dataset - Populate the dataset with the provided name with the provided audio files
  preprocess-dataset - Preprocess a dataset of audio files for training a voice model
  get-gpu-information - Retrieve information on locally available GPUs
  extract-features - Extract features from the preprocessed dataset associated with a voice model to be trained
  run-training - Train a voice model using its associated preprocessed dataset and extracted features

urvc generate - Generate audio using RVC
  wavify - Convert an audio track to wav format if its current format is an accepted format
  convert-voice - Convert a voice track using RVC
  song-cover - Generate song covers using RVC
  speech - Generate speech from text using RVC

Speech Generation Commands
urvc generate speech - Generate speech from text using RVC
  run-edge-tts - Convert text to speech using Edge TTS
  list-edge-tts-voices - List Edge TTS voices based on provided filters
  run-pipeline - Convert text to speech using a cascaded pipeline combining Edge TTS and RVC. The text is first converted to speech using Edge TTS, and then that speech is converted to a different voice using RVC

Voice Conversion Command
urvc generate convert-voice [voice_track] [directory] [model_name]

Required Arguments:
  voice_track (FILE) - The path to the voice track to convert
  directory (DIRECTORY) - The path to the directory where the converted voice track will be saved
  model_name (TEXT) - The name of the model to use for voice conversion

Main Options:
  --n-octaves (INTEGER) - The number of octaves to pitch-shift the converted voice by. Use 1 for male-to-female and -1 for vice-versa (default: 0)
  --n-semitones (INTEGER) - The number of semi-tones to pitch-shift the converted voice by. Altering this slightly reduces sound quality (default: 0)

Voice Synthesis Options:
  --f0-method [rmvpe|crepe|crepe-tiny|fcpe] - The method to use for pitch extraction. Can be provided multiple times to use multiple methods in combination. Defaults to rmvpe method (recommended)
  --index-rate (0-1) - The rate of influence of the index file. Increase to bias conversion towards the accent of the voice model. Decrease to potentially reduce artifacts (default: 0.3)
  --rms-mix-rate (0-1) - Blending rate for the volume envelope. Controls how much to mimic the loudness of the input voice (0) or a fixed loudness (1) (default: 1.0)
  --protect-rate (0-0.5) - Controls the extent to which consonants and breathing sounds are protected from artifacts. Higher value offers more protection but may worsen the indexing effect (default: 0.33)
  --hop-length (1-512) - Controls how often the CREPE-based pitch extraction method checks for pitch changes in milliseconds. Lower values lead to longer conversion times and higher risk of voice cracks, but better pitch accuracy (default: 128)

Voice Enrichment Options:
  --split-voice / --no-split-voice - Whether to split the voice track into smaller segments before converting. Can improve output quality for longer tracks (default: no-split-voice)
  --autotune-voice / --no-autotune-voice - Whether to apply autotune to the converted voice (default: no-autotune-voice)
  --autotune-strength (0-1) - The intensity of the autotune effect. Higher values result in stronger snapping to the chromatic grid (default: 1.0)
  --clean-voice / --no-clean-voice - Whether to clean the converted voice using noise reduction algorithms (default: no-clean-voice)
  --clean-strength (0-1) - The intensity of the cleaning to apply. Higher values result in stronger cleaning, but may lead to a more compressed sound (default: 0.7)

Speaker Embeddings Options:
  --embedder-model [contentvec|chinese-hubert-base|japanese-hubert-base|korean-hubert-base|custom] - The model to use for generating speaker embeddings (default: contentvec)
  --custom-embedder-model (TEXT) - The name of a custom embedder model to use. Only applicable if embedder-model is set to custom
  --sid (INTEGER) - The id of the speaker to use for multi-speaker RVC models (default: 0)

Edge TTS Command
urvc generate speech run-edge-tts [source]

Required Arguments:
  source (TEXT) - A string or path to a file containing the text to be converted

Options:
  --voice (TEXT) - The short name of the Edge TTS voice which should speak the provided text. Use list-edge-voices command to get available voices (default: en-US-ChristopherNeural)
  --pitch-shift (INTEGER) - The number of hertz to shift the pitch of the Edge TTS voice (default: 0)
  --speed-change (INTEGER) - The percentual change to the speed of the Edge TTS voice (default: 0)
  --volume-change (INTEGER) - The percentual change to the volume of the Edge TTS voice (default: 0)

Edge TTS + RVC Pipeline Command
urvc generate speech run-pipeline [source] [model_name]

Required Arguments:
  source (TEXT) - A string or path to a file containing the text to be converted
  model_name (TEXT) - The name of the RVC model to use for speech conversion

Edge TTS Options:
  --tts-voice (TEXT) - The short name of the Edge TTS voice to use for text-to-speech conversion (default: en-US-ChristopherNeural)
  --tts-pitch-shift (INTEGER) - The number of hertz to shift the pitch of the speech generated by Edge TTS (default: 0)
  --tts-speed-change (INTEGER) - The percentual change to the speed of the speech generated by Edge TTS (default: 0)
  --tts-volume-change (INTEGER) - The percentual change to the volume of the speech generated by Edge TTS (default: 0)

RVC Main Options:
  --n-octaves (INTEGER) - The number of octaves to shift the pitch of the speech converted using RVC. Use 1 for male-to-female and -1 for vice-versa (default: 0)
  --n-semitones (INTEGER) - The number of semitones to shift the pitch of the speech converted using RVC. Altering this slightly reduces sound quality (default: 0)

RVC Synthesis Options:
  --f0-method [rmvpe|crepe|crepe-tiny|fcpe] - The method to use for pitch extraction during the RVC process. Can be provided multiple times to use multiple methods in combination (default: rmvpe)
  --index-rate (0-1) - The rate of influence of the RVC index file. Increase to bias conversion towards the accent of the voice model. Decrease to potentially reduce artifacts (default: 0.3)
  --rms-mix-rate (0-1) - Blending rate for the volume envelope. Controls how much to mimic the loudness of the Edge TTS speech (0) or a fixed loudness (1) (default: 1.0)
  --protect-rate (0-0.5) - Controls the extent to which consonants and breathing sounds are protected from artifacts during RVC process. Higher value offers more protection but may worsen indexing effect (default: 0.33)
  --hop-length (1-512) - Controls how often the CREPE-based pitch extraction method checks for pitch changes. Lower values lead to longer conversion times and higher risk of voice cracks, but better pitch accuracy (default: 128)

RVC Enrichment Options:
  --split-speech / --no-split-speech - Whether to split the Edge TTS speech into smaller segments before converting using RVC. Can improve output quality for longer speech (default: split-speech)
  --autotune-speech / --no-autotune-speech - Whether to apply autotune to the speech converted using RVC (default: no-autotune-speech)
  --autotune-strength (0-1) - The intensity of the autotune effect. Higher values result in stronger snapping to the chromatic grid (default: 1.0)
  --clean-speech / --no-clean-speech - Whether to clean the speech converted using RVC using noise reduction algorithms (default: clean-speech)
  --clean-strength (0-1) - The intensity of the cleaning to apply to the speech converted using RVC. Higher values result in stronger cleaning, but may lead to more compressed sound (default: 0.7)

RVC Embeddings Options:
  --embedder-model [contentvec|chinese-hubert-base|japanese-hubert-base|korean-hubert-base|custom] - The model to use for generating speaker embeddings during RVC process (default: contentvec)
  --custom-embedder-model (TEXT) - The name of a custom embedder model to use for generating speaker embeddings. Only applicable if embedder-model is set to custom
  --sid (INTEGER) - The id of the speaker to use for multi-speaker RVC models (default: 0)

Audio Mixing Options:
  --output-gain (INTEGER) - The gain to apply to the converted speech during mixing (default: 0)
  --output-sr (INTEGER) - The sample rate of the mixed speech track (default: 44100)
  --output-format [mp3|wav|flac|ogg|m4a|aac] - The format of the mixed speech track (default: mp3)
  --output-name (TEXT) - The name of the mixed speech track (default: None)

Usage Examples

# Basic voice conversion
urvc generate convert-voice input.wav ./output/ my_voice_model

# Voice conversion with pitch shift (male to female)
urvc generate convert-voice input.wav ./output/ my_voice_model --n-octaves 1

# Voice conversion with enhanced quality settings
urvc generate convert-voice input.wav ./output/ my_voice_model --split-voice --clean-voice --autotune-voice

# Generate speech using Edge TTS only
urvc generate speech run-edge-tts "Hello world"

# Generate speech using Edge TTS with custom voice and settings
urvc generate speech run-edge-tts "Hello world" --voice en-US-JennyNeural --speed-change 10

# List available Edge TTS voices  
urvc generate speech list-edge-tts-voices

# Run full TTS + RVC pipeline (Edge TTS -> RVC conversion)
urvc generate speech run-pipeline "Hello world" "My_RVC_Model"

# Run pipeline with custom Edge TTS voice and RVC settings
urvc generate speech run-pipeline "Hello world" "My_RVC_Model" --tts-voice en-US-JennyNeural --f0-method crepe --clean-speech --output-format wav --output-name "my_speech"


Command
bashurvc train preprocess-dataset MODEL_NAME DATASET [OPTIONS]
Required Arguments
ArgumentTypeDescriptionMODEL_NAMETextName of the voice model to train. Creates new model if it doesn't exist, or replaces dataset for existing modelDATASETDirectoryPath to the folder containing audio files to preprocess
Options
Sample Rate
bash--sample-rate [32000|40000|48000]

Default: 40000
Purpose: Target sample rate for all audio files
Recommendation: Use 40000 for best RVC compatibility

Split Method
bash--split-method [Skip|Simple|Automatic]

Default: Automatic
Options:

Skip: Use if audio files are already properly split
Simple: Use if silence has been manually removed
Automatic: Let RVC detect and split around silence (recommended)



Chunk Settings (Simple method only)
bash--chunk-len 3.0          # Length of each audio chunk (0.5-5.0 seconds)
--overlap-len 0.3        # Overlap between chunks (0.0-0.4 seconds)
Audio Filtering
bash--filter-audio           # Apply high-pass filter (default: enabled)
--no-filter-audio        # Skip filtering

Purpose: Removes low-frequency noise and rumble
Recommendation: Keep enabled unless source audio is already clean

Audio Cleaning
bash--clean-audio            # Apply noise reduction
--no-clean-audio         # Skip cleaning (default)
--clean-strength 0.7     # Cleaning intensity (0.0-1.0)

Purpose: Reduces background noise and artifacts
Recommendation: Enable for lower quality source audio

Performance
bash--cpu-cores 20           # Number of CPU cores to use (1-20)

Default: 20
Recommendation: Use your CPU's thread count or slightly less

Examples
Basic Preprocessing
bashurvc train preprocess-dataset my_voice /path/to/audio/files
High-Quality Dataset (clean studio recordings)
bashurvc train preprocess-dataset my_voice /path/to/audio/files \
  --sample-rate 48000 \
  --split-method Skip \
  --no-filter-audio \
  --no-clean-audio
Low-Quality Dataset (phone recordings, background noise)
bashurvc train preprocess-dataset my_voice /path/to/audio/files \
  --sample-rate 40000 \
  --split-method Automatic \
  --filter-audio \
  --clean-audio \
  --clean-strength 0.8
Fast Processing (fewer CPU cores)
bashurvc train preprocess-dataset my_voice /path/to/audio/files \
  --cpu-cores 8
Tips

Dataset Quality: Higher quality input = better voice model
File Format: WAV files work best, avoid compressed formats
Duration: Aim for 10+ minutes of clean speech per model
Consistency: Use consistent recording conditions across all files
Content: Varied speech content (different words, emotions) improves model
Background: Remove background music and noise before preprocessing

Next Steps
After preprocessing, continue with:

urvc train extract-features - Extract voice characteristics
urvc train run-training - Train the actual voice model
Test with urvc generate convert-voice - Convert audio with your model